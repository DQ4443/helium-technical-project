# Backend Bug Findings and Improvements

## Summary

Found and fixed 5 major issues in the Go localization server, resulting in **6.7x throughput improvement** and fixing a critical crash bug.

## Issues Fixed (Ordered by Impact)

### 1. Redundant Redis Operations on Every Cache Hit (Performance + Crash)

**The biggest performance issue.** On every cache hit, the original code performed unnecessary operations:

```go
// ORIGINAL - on TTL cache hit
if cached, found := componentCache.Get(cacheKey); found {
    componentCache.Put(cacheKey, component)  // Unnecessary - Get() already updates LRU
    setInRedis(cacheKey, component)          // Unnecessary write + NO NIL CHECK = CRASH
}

// ORIGINAL - on Redis hit
component, err := getFromRedis(cacheKey)
if err == nil && component != nil {
    setInRedis(cacheKey, component)  // Just read it, now writing it back!
}
```

**Problems:**
- Every cache hit triggered a full Redis write (JSON serialize + network round-trip)
- The `setInRedis` call had no nil check - **server crashed when Redis was down** (98% failure rate in load test)
- Unnecessary `Put()` call since `Get()` already maintains LRU order

**Fix:** Removed redundant operations. Used `EXPIRE` command for Redis TTL refresh instead of full write.

**Impact:** 6.7x throughput improvement (1,265 â†’ 8,427 req/s)

---

### 2. Regex Compiled on Every Request (Performance)

```go
// ORIGINAL - compiled N times per request (once per localization key)
func interpolateTemplate(template string, localizedData map[string]string) string {
    for key, value := range localizedData {
        pattern := regexp.MustCompile(`\{l10n\.` + key + `\}`)  // Expensive!
        result = pattern.ReplaceAllString(result, value)
    }
}
```

**Problem:** Regex compilation is CPU-intensive. With 4 keys per component, this compiled 4 regexes per cache miss.

**Fix:** Pre-compile a single regex pattern at startup:
```go
var l10nPattern = regexp.MustCompile(`\{l10n\.([a-zA-Z_]+)\}`)
```

---

### 3. Cache Key Ignores Language Fallback (Correctness + Performance)

```go
// ORIGINAL
cacheKey := fmt.Sprintf("component:%s:%s", componentType, lang)  // Uses requested lang
// But if lang="zh" (unsupported), falls back to English internally
```

**Problems:**
- **Cache pollution:** Requests for `zh`, `ja`, `ko` each create separate cache entries storing identical English content
- **Stale cache risk:** If a language is later added, old English entries persist until TTL
- **Wrong response:** API returned `"language": "zh"` even when serving English

**Fix:** Normalize to actual supported language before caching:
```go
actualLang := lang
if _, exists := localizationDB[lang]; !exists {
    actualLang = "en"
}
cacheKey := fmt.Sprintf("component:%s:%s", componentType, actualLang)
```

---

### 4. No Timeout on Redis Operations (Reliability)

```go
// ORIGINAL - global context with no timeout
var ctx = context.Background()
val, err := redisClient.Get(ctx, key).Result()  // Can hang forever
```

**Problem:** If Redis becomes slow or unresponsive, requests hang indefinitely, causing goroutine exhaustion.

**Fix:** Use context with 2-second timeout for all Redis operations:
```go
ctx, cancel := context.WithTimeout(context.Background(), 2*time.Second)
defer cancel()
```

---

### 5. Health Endpoint Behind Concurrency Limiter (Operational)

```go
// ORIGINAL
router.Use(ConcurrencyLimiter(2))  // Applied globally
router.GET("/health", healthCheck)  // Health check gets 503 under load!
```

**Problem:** Load balancers receive 503 errors during high load, incorrectly marking healthy servers as down.

**Fix:** Register health endpoint before applying limiter middleware:
```go
router.GET("/health", healthCheck)  // Before limiter
api := router.Group("/api")
api.Use(ConcurrencyLimiter(2))
```

---

## Load Test Results

| Metric | Original | Fixed | Change |
|--------|----------|-------|--------|
| API Success Rate | 2% | 100% | Fixed crash |
| Throughput (c=2) | 1,265 req/s | 8,427 req/s | **6.7x faster** |
| Avg Latency | 1.6ms | 0.2ms | **8x faster** |
| Health under load | 98% (503s) | 100% | Fixed |

*Test environment: Redis disconnected, concurrency limit = 2*

### Key Findings

1. **Original server was completely broken** when Redis disconnected - 98% crash rate
2. **Fixed server handles Redis failure gracefully** - continues serving from memory cache
3. **6-7x throughput improvement** due to removing redundant Redis writes
4. **8x latency improvement** on cache hits

---

## Architecture Observations

### Design Decisions (Not Bugs)
1. **Lazy TTL expiration**: Items only expire when accessed. This is a valid design choice for simplicity, though it means stale items sit in memory.

2. **Non-blocking concurrency limiter**: Uses `select` with `default` case, meaning excess requests are immediately rejected rather than queued. This is intentional for load shedding.

3. **Two-tier cache**: In-memory LRU (10 min TTL) + Redis (30 min TTL) provides good latency with persistence.

### Recommendations
1. **Increase Concurrency Limit** - Current limit of 2 is very restrictive for production.
2. **Add Prometheus Metrics** - Track cache hit rates, latency percentiles, and 503 rates.
3. **Add CORS Headers** - Required if serving browser clients directly.
4. **Implement Request Queuing** - Consider brief queuing instead of immediate 503.
